{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Example\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To run this notebook, install the requirements listed in `requirements.txt` using Minconda3/Anaconda3 in a conda virtual enviroment and\n",
    "`conda create --name spark --file requirements.txt`. Then `source activate spark` to enable it.\n",
    "\n",
    "This does not install Spark to run in standalone mode. For that, you'll need to download and install the\n",
    "appropriate version of Spark for your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure spark drivers to work with Juypter\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# other imports and constants\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from pyspark.ml.feature import NGram, StopWordsRemover, Tokenizer\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "MOBY_BASE = 'https://www.gutenberg.org/files/2701/'\n",
    "MOBY_TXT = '2701-0.txt'\n",
    "MOBY_PATH = os.path.join(DATA_DIR, MOBY_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a text to analyze\n",
    "\n",
    "This notebook uses Apache Spark to do an n-gram analysis of Herman Melville's *Moby Dick*. First we need to define\n",
    "some functions to get the text from Project Gutenberg and then analyze it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moby():\n",
    "    \"\"\"Download the text to Moby Dick from Project Gutenberg,\n",
    "    if it does not already exist.\"\"\"\n",
    "    if not os.path.exists(os.path.join(DATA_DIR)):\n",
    "        sys.stdout.write('Creating download dir...\\n')\n",
    "        os.mkdir(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(MOBY_PATH):\n",
    "        res = requests.get('{}{}'.format(MOBY_BASE, MOBY_TXT))\n",
    "        if res.status_code != 200:\n",
    "            raise ValueError('Download did not complete successfully!')\n",
    "        with open(MOBY_PATH, 'w') as fp:\n",
    "            fp.write(res.text)\n",
    "        sys.stdout.write('Downloaded Moby Dick text to 2701-0.txt')\n",
    "    else:\n",
    "        sys.stdout.write('Text file exists... skipping.\\n')\n",
    "\n",
    "\n",
    "def get_sentences():\n",
    "    \"\"\"Open the downloaded file and use nltk to split it up by sentence,\n",
    "    stripping Project Gutenberg headers.\"\"\"\n",
    "    with open(MOBY_PATH, 'r') as fp:\n",
    "        # this remove Gutenberg header and footer\n",
    "        lines = [l.strip('\\n') for l in fp.readlines()[848:21964]]\n",
    "        sentences = PunktSentenceTokenizer().sentences_from_text(' '.join(lines))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # format as tuples with id and string, expected format of\n",
    "            # createDataFrame below\n",
    "            sentences[i] = (i, re.sub(r'[^\\w\\s]','', sentence))\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the analysis\n",
    "\n",
    "Here we use several Spark functions to get bigrams, trigrams, and quadgrams, which can be a very punishing\n",
    "process unless run in parallel.\n",
    "\n",
    "The code creates a SparkSession, which gives us access to the SparkContext (the SQL interface) as well as the older RDD interface to Spark (now somewhat deprecated).\n",
    "\n",
    "It then runs a tokenizer over each sentences to split by word, and then NGram analysers. Finally, we write the coalesced result to a JSON file for easy traversal and a CSV of bigrams, to take a look at our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic cleaning and getting of files\n",
    "get_moby()\n",
    "sentences = get_sentences()\n",
    "\n",
    "# create spark app, for use in iPython notebook OR as a standalone.\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"NGramSample\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# build a distributed dataframe\n",
    "sentence_df = spark.createDataFrame(sentences, ['id', 'sentences'])\n",
    "\n",
    "# create a tokenizer and write a 'words' column to DF\n",
    "tokenizer = Tokenizer(inputCol='sentences', outputCol='words')\n",
    "words = tokenizer.transform(sentence_df)\n",
    "\n",
    "# create ngram generators for bi, tri, and quad grams\n",
    "bigram = NGram(n=2, inputCol='words', outputCol='bigrams')\n",
    "trigram = NGram(n=3, inputCol='words', outputCol='trigrams')\n",
    "quadgram = NGram(n=4, inputCol='words', outputCol='quadgrams')\n",
    "\n",
    "# add each one in turn to the df\n",
    "bigrams = bigram.transform(words)\n",
    "trigrams = trigram.transform(bigrams)\n",
    "final = quadgram.transform(trigrams)\n",
    "\n",
    "# write as traversable JSON\n",
    "if os.path.exists('ngrams'):\n",
    "    shutil.rmtree('ngrams')\n",
    "final.coalesce(1).write.json('ngrams')\n",
    "\n",
    "# as an example, write out quadgrams to CSV\n",
    "if os.path.exists('bigrams'):\n",
    "    shutil.rmtree('bigrams')\n",
    "\n",
    "# This tricky bit selects bigrams, explodes it, and regroups by unique\n",
    "# bigram, then adds a count, after filtering out extremely uncommon bigrams\n",
    "# It finally writes to a CSV\n",
    "final.select('bigrams')\\\n",
    "    .withColumn('bigrams', explode('bigrams'))\\\n",
    "    .groupBy('bigrams').count().orderBy('count', ascending=False)\\\n",
    "    .filter('count > 10')\\\n",
    "    .coalesce(1).write.csv('bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also display dataframe columns easily\n",
    "final.select('sentences', 'bigrams', 'trigrams', 'quadgrams').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
